{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logistic regression model to classify movie reviews from the 50k IMDb review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ghost/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /home/ghost/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ghost/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import time \n",
    "import pandas\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, opinion_lexicon\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "np.random.seed(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last five reviews from the 50k IMBb review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Builing a dictionary with positive and negative opinions with a position on each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = opinion_lexicon.negative()\n",
    "pos_words = opinion_lexicon.positive()\n",
    "\n",
    "pos_dict = {}\n",
    "neg_dict = {}\n",
    "\n",
    "npos = len(pos_words)\n",
    "nneg = len(neg_words)\n",
    "\n",
    "for ipos, pos_word in enumerate(pos_words): pos_dict[pos_word] = ipos + 29\n",
    "for ineg, neg_word in enumerate(neg_words): neg_dict[neg_word] = ineg + npos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list with pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop     = stopwords.words('english')\n",
    "pronouns = stop[:35]\n",
    "not_pr   = stop[35:]\n",
    "\n",
    "pronouns1 = pronouns[:8]\n",
    "pronouns1.append('us')\n",
    "pronouns2 = pronouns[8:17]\n",
    "pronouns3 = pronouns[17:]\n",
    "\n",
    "not_pr.remove('no')\n",
    "not_pr.remove('not')\n",
    "not_pr.remove('nor')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a tokenizer to split the text into word tokens. Also we remove HTML tags, emoticons and all words that not belong to the pronouns set. Finally with apply the Porter stemming and WordNet Lemmatizer algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text      = re.sub('<[^>]*>', '', text)\n",
    "    #emoticons = re.findall('(?::|;|=|x|8|\\()(?:-|y)?(?:\\)|s|\\(|\\)|d|p|c|3|\\[|\\]|\\||\\\\\\\\|\\\\/)', text.lower())\n",
    "    text      = re.sub(    '(?::|;|=|x|8|\\()(?:-|y)?(?:\\)|s|\\(|\\)|d|p|c|3|\\[|\\]|\\||\\\\\\\\|\\\\/)', '',text.lower())\n",
    "    text      = re.sub('[\\s\\?\\[\\]\\,\\;\\.\\:\\-\\\\_\\(\\)\\\"]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text      = [w for w in text.split() if w not in not_pr]\n",
    "    tokenized = []\n",
    "    for w in text:\n",
    "        if w in neg_dict or w in pos_dict:\n",
    "            tokenized.append(w)\n",
    "        else:\n",
    "            w_s = porter.stem(w)\n",
    "            if w_s in neg_dict or w in pos_dict:\n",
    "                tokenized.append(w_s)\n",
    "            else:\n",
    "                w_l = wordnet_lemmatizer.lemmatize(w)\n",
    "                tokenized.append(w_l)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a DocToVec class to convert all documents to a feature vector using a simple tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excercise 1: define new features according to https://web.stanford.edu/~jurafsky/slp3/5.pdf\n",
    "\n",
    "class DocToVec:\n",
    "    \"\"\"\n",
    "        Convert a list of words to feature vectors:\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            n_features: number of features per vector, by default 6818.\n",
    "            fn        : tokenizer function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, fn):\n",
    "        if n_features is None:\n",
    "            self.n_features = 29\n",
    "            self.fn         = tokenizer\n",
    "        self.n_features = n_features\n",
    "        self.fn         = fn\n",
    "    def transform(self, DocList):\n",
    "    \"\"\"\n",
    "        Transform an array of list of words to a matrix:\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            DocList: Array of list of Words tokenized\n",
    "            \n",
    "        Returns:\n",
    "            \n",
    "            Matrix with feature vectors of each review.\n",
    "            \n",
    "        feature0: Word Length\n",
    "        feature1: Expression ! count\n",
    "        feature2: Positive Words count\n",
    "        feature3: Negative Words count\n",
    "        feature4: Pronouns of 1st person count \n",
    "        feature5: Pronouns of 2nd person count \n",
    "        feature6: Pronouns of 3rd person count \n",
    "        feature7: Words like 'no' count\n",
    "        feature8: Words like 'not' count\n",
    "        feature9: Words like 'nor' or 'neither' count\n",
    "        \n",
    "        feature10 - feature28: Combinations of some feature that have a good correlation\n",
    "        feature29 - end      : All Positive and Negative words \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "        n   = len(DocList)\n",
    "        vec = np.zeros(shape=(n, self.n_features), dtype = np.float32)\n",
    "        mt  = 0.0\n",
    "        for idx, strn in enumerate(DocList):\n",
    "            st = time.time()\n",
    "            strnList = self.fn(strn)\n",
    "            vec[idx][0] = np.log(len(strnList))\n",
    "            for strn in strnList:\n",
    "                exp      = re.findall('!', strn)\n",
    "                \n",
    "                if len(exp) != 0:\n",
    "                    strn         = re.sub('!','',strn)\n",
    "                    vec[idx][1] += len(exp) \n",
    "                    \n",
    "                if strn in pos_dict:\n",
    "                    vec[idx][2] += 1 \n",
    "                    vec[idx][pos_dict[strn]] +=1\n",
    "                    \n",
    "                elif strn in neg_dict:\n",
    "                    vec[idx][3] += 1 \n",
    "                    vec[idx][neg_dict[strn]] +=1\n",
    "\n",
    "                elif strn in pronouns1:\n",
    "                    vec[idx][4] += 1  \n",
    "\n",
    "                elif strn in pronouns2:\n",
    "                    vec[idx][5] += 1 \n",
    "                \n",
    "                elif strn in pronouns3:\n",
    "                    vec[idx][6] += 1 \n",
    "                \n",
    "                elif strn == 'no':\n",
    "                    vec[idx][7] += 1\n",
    "                \n",
    "                elif strn == 'not':\n",
    "                    vec[idx][8] += 1\n",
    "                    \n",
    "                elif strn in ['nor','neither']:\n",
    "                    vec[idx][9] += 1\n",
    "                    \n",
    "                vec[idx][10] = vec[idx][2] * vec[idx][3]\n",
    "\n",
    "                vec[idx][11] = vec[idx][2] * vec[idx][4]\n",
    "                vec[idx][12] = vec[idx][2] * vec[idx][5]\n",
    "                vec[idx][13] = vec[idx][2] * vec[idx][6]\n",
    "                vec[idx][14] = vec[idx][2] * vec[idx][7]\n",
    "                vec[idx][15] = vec[idx][2] * vec[idx][8]  \n",
    "                vec[idx][16] = vec[idx][2] * vec[idx][9]  \n",
    "                \n",
    "                vec[idx][17] = vec[idx][3] * vec[idx][4]\n",
    "                vec[idx][18] = vec[idx][3] * vec[idx][5]\n",
    "                vec[idx][19] = vec[idx][3] * vec[idx][6]\n",
    "                vec[idx][20] = vec[idx][3] * vec[idx][7]\n",
    "                vec[idx][21] = vec[idx][3] * vec[idx][8]  \n",
    "                vec[idx][22] = vec[idx][3] * vec[idx][9]  \n",
    "                \n",
    "            vec[idx][23] = vec[idx][2]**2\n",
    "            vec[idx][24] = vec[idx][3]**2\n",
    "            vec[idx][25] = vec[idx][2]**3\n",
    "            vec[idx][26] = vec[idx][3]**3\n",
    "            vec[idx][27] = np.sqrt(vec[idx][2])\n",
    "            vec[idx][28] = np.sqrt(vec[idx][3])\n",
    "            dt = time.time() - st\n",
    "            mt+= dt\n",
    "            ln = 36*(idx + 1)\n",
    "            \n",
    "            if ((idx+1) % 500 == 0):\n",
    "                print('percent = {:4.1f}%, time estimated : {:3.1f} min'.format(100*(idx+1)/n, mt*n/ln))\n",
    "        return vec        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions below were not used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    \n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "            \n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    \n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6818)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Exercise 2: implement a Logistic Regression classifier, using regularization\n",
    "\n",
    "FEATURES = 29 + npos + nneg\n",
    "vect = DocToVec(n_features = FEATURES, fn = tokenizer)\n",
    "\n",
    "vect.transform([df.head().values[0][0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent =  2.0%, time estimated : 5.9 min\n",
      "percent =  4.0%, time estimated : 5.8 min\n",
      "percent =  6.0%, time estimated : 5.8 min\n",
      "percent =  8.0%, time estimated : 5.8 min\n",
      "percent = 10.0%, time estimated : 5.8 min\n",
      "percent = 12.0%, time estimated : 5.8 min\n",
      "percent = 14.0%, time estimated : 5.8 min\n",
      "percent = 16.0%, time estimated : 5.8 min\n",
      "percent = 18.0%, time estimated : 5.8 min\n",
      "percent = 20.0%, time estimated : 5.8 min\n",
      "percent = 22.0%, time estimated : 5.8 min\n",
      "percent = 24.0%, time estimated : 5.8 min\n",
      "percent = 26.0%, time estimated : 5.8 min\n",
      "percent = 28.0%, time estimated : 5.8 min\n",
      "percent = 30.0%, time estimated : 5.8 min\n",
      "percent = 32.0%, time estimated : 5.8 min\n",
      "percent = 34.0%, time estimated : 5.8 min\n",
      "percent = 36.0%, time estimated : 5.8 min\n",
      "percent = 38.0%, time estimated : 5.8 min\n",
      "percent = 40.0%, time estimated : 5.8 min\n",
      "percent = 42.0%, time estimated : 5.8 min\n",
      "percent = 44.0%, time estimated : 5.8 min\n",
      "percent = 46.0%, time estimated : 5.8 min\n",
      "percent = 48.0%, time estimated : 5.8 min\n",
      "percent = 50.0%, time estimated : 5.8 min\n",
      "percent = 52.0%, time estimated : 5.8 min\n",
      "percent = 54.0%, time estimated : 5.8 min\n",
      "percent = 56.0%, time estimated : 5.8 min\n",
      "percent = 58.0%, time estimated : 5.8 min\n",
      "percent = 60.0%, time estimated : 5.8 min\n",
      "percent = 62.0%, time estimated : 5.8 min\n",
      "percent = 64.0%, time estimated : 5.8 min\n",
      "percent = 66.0%, time estimated : 5.8 min\n",
      "percent = 68.0%, time estimated : 5.8 min\n",
      "percent = 70.0%, time estimated : 5.9 min\n",
      "percent = 72.0%, time estimated : 5.9 min\n",
      "percent = 74.0%, time estimated : 5.9 min\n",
      "percent = 76.0%, time estimated : 5.9 min\n",
      "percent = 78.0%, time estimated : 5.9 min\n",
      "percent = 80.0%, time estimated : 5.9 min\n",
      "percent = 82.0%, time estimated : 5.9 min\n",
      "percent = 84.0%, time estimated : 5.9 min\n",
      "percent = 86.0%, time estimated : 5.9 min\n",
      "percent = 88.0%, time estimated : 6.0 min\n",
      "percent = 90.0%, time estimated : 6.0 min\n",
      "percent = 92.0%, time estimated : 5.9 min\n",
      "percent = 94.0%, time estimated : 6.0 min\n",
      "percent = 96.0%, time estimated : 6.0 min\n",
      "percent = 98.0%, time estimated : 6.0 min\n",
      "percent = 100.0%, time estimated : 6.0 min\n"
     ]
    }
   ],
   "source": [
    "stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "\n",
    "X_train, y_train = get_minibatch(stream, size=50000)\n",
    "y_train = np.asarray(y_train).reshape((-1, 1))\n",
    "\n",
    "X_train = vect.transform(X_train)\n",
    "\n",
    "np.save('vec_data_X', X_train)\n",
    "np.save('vec_data_y', y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: implement a Logistic Regression classifier, using regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 6818)\n",
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_data_r = normalize(np.load('vec_data_X.npy'), axis=0)\n",
    "y_data_r = np.load('vec_data_y.npy')\n",
    "\n",
    "permut   = np.random.permutation(X_data_r.shape[0])\n",
    "\n",
    "X_data_r = X_data_r[permut]\n",
    "y_data_r = y_data_r[permut]\n",
    "\n",
    "print(X_data_r.shape)\n",
    "print(y_data_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix was used to analyze the depence of the fist 29 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment            1.000000\n",
       "negative_words_sq    0.261996\n",
       "positive_words_sq    0.240795\n",
       "positive_words       0.215529\n",
       "negative_words       0.202699\n",
       "positive_words_2     0.154977\n",
       "no_count             0.145020\n",
       "neg_no               0.115677\n",
       "neg_pronoun1st       0.114881\n",
       "negative_words_2     0.111458\n",
       "positive_words_3     0.104311\n",
       "pos_pronoun3rd       0.097813\n",
       "neg_pronoun2nd       0.096465\n",
       "neg_not              0.072669\n",
       "negative_words_3     0.059022\n",
       "pos_pronoun1st       0.057093\n",
       "not_count            0.053531\n",
       "pos_not              0.048238\n",
       "nor_neither          0.045743\n",
       "pronouns3rd          0.044133\n",
       "neg_nor_neither      0.041784\n",
       "neg_pronoun3rd       0.039384\n",
       "pos_pronoun2nd       0.033470\n",
       "pronouns2nd          0.032456\n",
       "pos_no               0.031448\n",
       "pronouns1st          0.030964\n",
       "count_!              0.013676\n",
       "pos_nor_neither      0.012478\n",
       "pos_neg              0.005169\n",
       "wordCount_log        0.001010\n",
       "Name: Sentiment, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Sentiment': y_data_r[:,0]})\n",
    "\n",
    "df['wordCount_log' ]    = X_data_r[:,0 ]\n",
    "df['count_!'       ]    = X_data_r[:,1 ]\n",
    "df['positive_words']    = X_data_r[:,2 ]\n",
    "df['negative_words']    = X_data_r[:,3 ]\n",
    "df['pronouns1st'   ]    = X_data_r[:,4 ]\n",
    "df['pronouns2nd'   ]    = X_data_r[:,5 ]\n",
    "df['pronouns3rd'   ]    = X_data_r[:,6 ]\n",
    "df['no_count'      ]    = X_data_r[:,7 ]\n",
    "df['not_count'     ]    = X_data_r[:,8 ]\n",
    "df['nor_neither'   ]    = X_data_r[:,9 ]\n",
    "\n",
    "df['pos_neg'       ]    = X_data_r[:,10]\n",
    "\n",
    "df['pos_pronoun1st']    = X_data_r[:,11]\n",
    "df['pos_pronoun2nd']    = X_data_r[:,12]\n",
    "df['pos_pronoun3rd']    = X_data_r[:,13]\n",
    "df['pos_no'        ]    = X_data_r[:,14]\n",
    "df['pos_not'       ]    = X_data_r[:,15]\n",
    "df['pos_nor_neither']   = X_data_r[:,16]\n",
    "\n",
    "df['neg_pronoun1st']    = X_data_r[:,17]\n",
    "df['neg_pronoun2nd']    = X_data_r[:,18]\n",
    "df['neg_pronoun3rd']    = X_data_r[:,19]\n",
    "df['neg_no'        ]    = X_data_r[:,20]\n",
    "df['neg_not'       ]    = X_data_r[:,21]\n",
    "df['neg_nor_neither']   = X_data_r[:,22]\n",
    "\n",
    "df['positive_words_2' ] = X_data_r[:,23]\n",
    "df['negative_words_2' ] = X_data_r[:,24]\n",
    "df['positive_words_3' ] = X_data_r[:,25]\n",
    "df['negative_words_3' ] = X_data_r[:,26]\n",
    "df['positive_words_sq'] = X_data_r[:,27]\n",
    "df['negative_words_sq'] = X_data_r[:,28]\n",
    "\n",
    "abs(df.corr()['Sentiment']).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 6818) (40000, 1)\n",
      "(5000, 6818) (5000, 1)\n",
      "(5000, 6818) (5000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_data_train = X_data_r[:40000]\n",
    "y_data_train = y_data_r[:40000]\n",
    "\n",
    "X_data_valid = X_data_r[40000:45000]\n",
    "y_data_valid = y_data_r[40000:45000]\n",
    "\n",
    "X_data_test  = X_data_r[45000:]\n",
    "y_data_test  = y_data_r[45000:]\n",
    "\n",
    "print(X_data_train.shape, y_data_train.shape)\n",
    "print(X_data_valid.shape, y_data_valid.shape)\n",
    "print(X_data_test.shape, y_data_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple class to train the logistic regression with L1 and L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_regr:\n",
    "    def __init__ (self, W, b, alpha):\n",
    "        self.W       = W\n",
    "        self.b       = b\n",
    "        self.alpha   = alpha\n",
    "        self.l2_coef = 5e-8\n",
    "        self.l1_coef = 5e-8\n",
    "    def sigmoid(self,z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def pred(self, X):\n",
    "        return self.sigmoid(np.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def acc(self, X, y):\n",
    "        pred_lim = self.pred(X) > 0.5\n",
    "        return np.mean(pred_lim == y)\n",
    "    \n",
    "    def cross_entropy(self, X, y):\n",
    "        return - np.mean(np.multiply(y  , np.log(self.pred(X)     + 1e-8))\n",
    "                       + np.multiply(1-y, np.log(1 - self.pred(X) + 1e-8))) + \\\n",
    "                    self.l2_coef * np.mean(np.square(self.W)) / 2 + \\\n",
    "                    self.l1_coef * np.mean(np.abs(self.W))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.W = self.W - self.alpha * np.matmul(np.transpose(X), self.pred(X) - y) / X.shape[0] - \\\n",
    "                          self.alpha * self.l2_coef * self.W - \\\n",
    "                          self.alpha * self.l1_coef * np.multiply(self.W, 1 / np.abs(self.W))\n",
    "        self.b = self.b - self.alpha * np.mean(self.pred(X) - y)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1 epoch ,  19956.20 docs/s, loss = 0.673469, loss_val = 0.675154, acc = 0.560\n",
      "     2 epoch ,  19481.36 docs/s, loss = 0.655189, loss_val = 0.658764, acc = 0.665\n",
      "     3 epoch ,  18150.50 docs/s, loss = 0.638594, loss_val = 0.643865, acc = 0.731\n",
      "     4 epoch ,  13625.10 docs/s, loss = 0.623479, loss_val = 0.630275, acc = 0.773\n",
      "     5 epoch ,  13197.06 docs/s, loss = 0.609659, loss_val = 0.617837, acc = 0.794\n",
      "     6 epoch ,  18857.97 docs/s, loss = 0.596974, loss_val = 0.606410, acc = 0.807\n",
      "     7 epoch ,  17514.24 docs/s, loss = 0.585286, loss_val = 0.595876, acc = 0.816\n",
      "     8 epoch ,  19866.07 docs/s, loss = 0.574477, loss_val = 0.586133, acc = 0.823\n",
      "     9 epoch ,  20145.87 docs/s, loss = 0.564448, loss_val = 0.577092, acc = 0.827\n",
      "    10 epoch ,  20189.90 docs/s, loss = 0.555113, loss_val = 0.568678, acc = 0.832\n",
      "    11 epoch ,  20142.92 docs/s, loss = 0.546397, loss_val = 0.560826, acc = 0.837\n",
      "    12 epoch ,  20048.99 docs/s, loss = 0.538238, loss_val = 0.553480, acc = 0.840\n",
      "    13 epoch ,  20282.22 docs/s, loss = 0.530580, loss_val = 0.546589, acc = 0.842\n",
      "    14 epoch ,  20082.32 docs/s, loss = 0.523375, loss_val = 0.540112, acc = 0.843\n",
      "    15 epoch ,  20238.20 docs/s, loss = 0.516582, loss_val = 0.534011, acc = 0.844\n",
      "    16 epoch ,  18586.00 docs/s, loss = 0.510164, loss_val = 0.528253, acc = 0.845\n",
      "    17 epoch ,  17343.42 docs/s, loss = 0.504088, loss_val = 0.522808, acc = 0.845\n",
      "    18 epoch ,  20138.30 docs/s, loss = 0.498326, loss_val = 0.517651, acc = 0.845\n",
      "    19 epoch ,  20101.09 docs/s, loss = 0.492852, loss_val = 0.512759, acc = 0.847\n",
      "    20 epoch ,  19672.76 docs/s, loss = 0.487643, loss_val = 0.508110, acc = 0.848\n",
      "    21 epoch ,  17810.06 docs/s, loss = 0.482680, loss_val = 0.503687, acc = 0.849\n",
      "    22 epoch ,  20043.55 docs/s, loss = 0.477943, loss_val = 0.499473, acc = 0.850\n",
      "    23 epoch ,  19911.26 docs/s, loss = 0.473417, loss_val = 0.495454, acc = 0.850\n",
      "    24 epoch ,  19948.48 docs/s, loss = 0.469087, loss_val = 0.491614, acc = 0.851\n",
      "    25 epoch ,  19532.66 docs/s, loss = 0.464939, loss_val = 0.487943, acc = 0.851\n",
      "    26 epoch ,  14758.58 docs/s, loss = 0.460961, loss_val = 0.484429, acc = 0.852\n",
      "    27 epoch ,  12286.33 docs/s, loss = 0.457142, loss_val = 0.481063, acc = 0.852\n",
      "    28 epoch ,  13035.18 docs/s, loss = 0.453472, loss_val = 0.477834, acc = 0.853\n",
      "    29 epoch ,  11995.20 docs/s, loss = 0.449941, loss_val = 0.474734, acc = 0.854\n",
      "    30 epoch ,  12937.22 docs/s, loss = 0.446542, loss_val = 0.471755, acc = 0.854\n",
      "    31 epoch ,  13388.05 docs/s, loss = 0.443267, loss_val = 0.468891, acc = 0.854\n",
      "    32 epoch ,  15037.33 docs/s, loss = 0.440107, loss_val = 0.466135, acc = 0.855\n",
      "    33 epoch ,  16185.25 docs/s, loss = 0.437057, loss_val = 0.463481, acc = 0.855\n",
      "    34 epoch ,  14374.83 docs/s, loss = 0.434111, loss_val = 0.460923, acc = 0.855\n",
      "    35 epoch ,  13825.89 docs/s, loss = 0.431263, loss_val = 0.458455, acc = 0.855\n",
      "    36 epoch ,  13123.48 docs/s, loss = 0.428508, loss_val = 0.456074, acc = 0.855\n",
      "    37 epoch ,  15088.96 docs/s, loss = 0.425841, loss_val = 0.453774, acc = 0.856\n",
      "    38 epoch ,  17780.56 docs/s, loss = 0.423257, loss_val = 0.451552, acc = 0.856\n",
      "    39 epoch ,  18397.06 docs/s, loss = 0.420752, loss_val = 0.449403, acc = 0.856\n",
      "    40 epoch ,  14277.67 docs/s, loss = 0.418323, loss_val = 0.447325, acc = 0.856\n",
      "    41 epoch ,  12624.78 docs/s, loss = 0.415966, loss_val = 0.445313, acc = 0.856\n",
      "    42 epoch ,  15944.07 docs/s, loss = 0.413677, loss_val = 0.443365, acc = 0.857\n",
      "    43 epoch ,  16302.77 docs/s, loss = 0.411453, loss_val = 0.441477, acc = 0.857\n",
      "    44 epoch ,  15409.61 docs/s, loss = 0.409292, loss_val = 0.439647, acc = 0.857\n",
      "    45 epoch ,  17486.48 docs/s, loss = 0.407190, loss_val = 0.437872, acc = 0.857\n",
      "    46 epoch ,  16565.59 docs/s, loss = 0.405145, loss_val = 0.436150, acc = 0.857\n",
      "    47 epoch ,  17616.17 docs/s, loss = 0.403154, loss_val = 0.434478, acc = 0.858\n",
      "    48 epoch ,  15023.05 docs/s, loss = 0.401215, loss_val = 0.432855, acc = 0.858\n",
      "    49 epoch ,  14368.29 docs/s, loss = 0.399326, loss_val = 0.431278, acc = 0.857\n",
      "    50 epoch ,  15924.46 docs/s, loss = 0.397485, loss_val = 0.429745, acc = 0.857\n",
      "    51 epoch ,  15222.63 docs/s, loss = 0.395689, loss_val = 0.428255, acc = 0.857\n",
      "    52 epoch ,  15347.76 docs/s, loss = 0.393938, loss_val = 0.426806, acc = 0.857\n",
      "    53 epoch ,  17148.26 docs/s, loss = 0.392229, loss_val = 0.425396, acc = 0.857\n",
      "    54 epoch ,  13983.83 docs/s, loss = 0.390561, loss_val = 0.424023, acc = 0.856\n",
      "    55 epoch ,  13999.37 docs/s, loss = 0.388931, loss_val = 0.422687, acc = 0.857\n",
      "    56 epoch ,  16940.02 docs/s, loss = 0.387340, loss_val = 0.421386, acc = 0.856\n",
      "    57 epoch ,  15625.92 docs/s, loss = 0.385784, loss_val = 0.420118, acc = 0.856\n",
      "    58 epoch ,  16356.77 docs/s, loss = 0.384264, loss_val = 0.418883, acc = 0.856\n",
      "    59 epoch ,  15968.64 docs/s, loss = 0.382777, loss_val = 0.417679, acc = 0.856\n",
      "    60 epoch ,  16932.05 docs/s, loss = 0.381323, loss_val = 0.416505, acc = 0.856\n",
      "    61 epoch ,  17016.89 docs/s, loss = 0.379900, loss_val = 0.415360, acc = 0.857\n",
      "    62 epoch ,  15526.26 docs/s, loss = 0.378507, loss_val = 0.414242, acc = 0.856\n",
      "    63 epoch ,  15266.42 docs/s, loss = 0.377144, loss_val = 0.413152, acc = 0.857\n",
      "    64 epoch ,   9543.06 docs/s, loss = 0.375808, loss_val = 0.412088, acc = 0.857\n",
      "    65 epoch ,  15351.26 docs/s, loss = 0.374500, loss_val = 0.411049, acc = 0.857\n",
      "    66 epoch ,  13439.70 docs/s, loss = 0.373218, loss_val = 0.410034, acc = 0.857\n",
      "    67 epoch ,  14776.24 docs/s, loss = 0.371962, loss_val = 0.409043, acc = 0.857\n",
      "    68 epoch ,  16704.32 docs/s, loss = 0.370731, loss_val = 0.408075, acc = 0.857\n",
      "    69 epoch ,  17858.86 docs/s, loss = 0.369523, loss_val = 0.407129, acc = 0.857\n",
      "    70 epoch ,  19296.90 docs/s, loss = 0.368339, loss_val = 0.406204, acc = 0.857\n",
      "    71 epoch ,  18319.00 docs/s, loss = 0.367177, loss_val = 0.405300, acc = 0.857\n",
      "    72 epoch ,  16323.98 docs/s, loss = 0.366037, loss_val = 0.404416, acc = 0.858\n",
      "    73 epoch ,  13941.89 docs/s, loss = 0.364918, loss_val = 0.403551, acc = 0.858\n",
      "    74 epoch ,  16164.64 docs/s, loss = 0.363820, loss_val = 0.402705, acc = 0.857\n",
      "    75 epoch ,  17429.97 docs/s, loss = 0.362741, loss_val = 0.401878, acc = 0.858\n",
      "    76 epoch ,  18701.17 docs/s, loss = 0.361682, loss_val = 0.401068, acc = 0.858\n",
      "    77 epoch ,  19352.91 docs/s, loss = 0.360641, loss_val = 0.400275, acc = 0.858\n",
      "    78 epoch ,  18860.85 docs/s, loss = 0.359619, loss_val = 0.399499, acc = 0.858\n",
      "    79 epoch ,  17750.34 docs/s, loss = 0.358615, loss_val = 0.398740, acc = 0.858\n",
      "    80 epoch ,  19970.50 docs/s, loss = 0.357627, loss_val = 0.397996, acc = 0.859\n",
      "    81 epoch ,  19407.64 docs/s, loss = 0.356657, loss_val = 0.397267, acc = 0.859\n",
      "    82 epoch ,  18421.64 docs/s, loss = 0.355702, loss_val = 0.396553, acc = 0.859\n",
      "    83 epoch ,  19271.64 docs/s, loss = 0.354764, loss_val = 0.395854, acc = 0.859\n",
      "    84 epoch ,  20016.53 docs/s, loss = 0.353841, loss_val = 0.395169, acc = 0.859\n",
      "    85 epoch ,  17287.96 docs/s, loss = 0.352933, loss_val = 0.394497, acc = 0.859\n",
      "    86 epoch ,  14723.79 docs/s, loss = 0.352039, loss_val = 0.393839, acc = 0.859\n",
      "    87 epoch ,  14711.92 docs/s, loss = 0.351160, loss_val = 0.393194, acc = 0.859\n",
      "    88 epoch ,  16281.03 docs/s, loss = 0.350295, loss_val = 0.392561, acc = 0.859\n",
      "    89 epoch ,  18323.61 docs/s, loss = 0.349443, loss_val = 0.391941, acc = 0.859\n",
      "    90 epoch ,  15863.09 docs/s, loss = 0.348605, loss_val = 0.391333, acc = 0.859\n",
      "    91 epoch ,  15066.73 docs/s, loss = 0.347779, loss_val = 0.390736, acc = 0.859\n",
      "    92 epoch ,  13883.81 docs/s, loss = 0.346966, loss_val = 0.390151, acc = 0.859\n",
      "    93 epoch ,  12987.92 docs/s, loss = 0.346165, loss_val = 0.389577, acc = 0.859\n",
      "    94 epoch ,  14239.69 docs/s, loss = 0.345376, loss_val = 0.389013, acc = 0.859\n",
      "    95 epoch ,  16701.05 docs/s, loss = 0.344598, loss_val = 0.388461, acc = 0.859\n",
      "    96 epoch ,  17626.47 docs/s, loss = 0.343832, loss_val = 0.387918, acc = 0.859\n",
      "    97 epoch ,  19789.78 docs/s, loss = 0.343078, loss_val = 0.387386, acc = 0.859\n",
      "    98 epoch ,  18573.72 docs/s, loss = 0.342334, loss_val = 0.386863, acc = 0.860\n",
      "    99 epoch ,  17390.57 docs/s, loss = 0.341600, loss_val = 0.386350, acc = 0.859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100 epoch ,  12661.78 docs/s, loss = 0.340877, loss_val = 0.385846, acc = 0.860\n"
     ]
    }
   ],
   "source": [
    "W_ = np.random.normal(0, 0.01, (FEATURES,1))\n",
    "b_ = 0.0\n",
    "lr = 0.5\n",
    "\n",
    "regr  = log_regr(W_, b_, lr)\n",
    "\n",
    "BATCH = 10\n",
    "STEP  = 1\n",
    "fmt   = '{:6d} epoch , {:9.2f} docs/s, loss = {:7.6f}, loss_val = {:7.6f}, acc = {:4.3f}'\n",
    "for i in range(100):\n",
    "    loss_t  = 0.0\n",
    "    n_batch = X_data_train.shape[0]/BATCH\n",
    "    st = time.time()\n",
    "    for i_batch in range(0,X_data_train.shape[0], BATCH):\n",
    "        X_batch = X_data_train[i_batch:i_batch+BATCH]\n",
    "        y_batch = y_data_train[i_batch:i_batch+BATCH]\n",
    "        regr.fit(X_batch , y_batch)\n",
    "        loss_t  += regr.cross_entropy(X_batch, y_batch)\n",
    "    dt   = time.time() - st\n",
    "        \n",
    "    if (i+1)%STEP == 0:\n",
    "        print(fmt.format((i+1), (X_data_train.shape[0])/dt,\n",
    "                                      loss_t/n_batch, \n",
    "                                      regr.cross_entropy(X_data_valid, y_data_valid),\n",
    "                                      regr.acc(X_data_valid, y_data_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc = 0.85860\n"
     ]
    }
   ],
   "source": [
    "print('acc = {:6.5f}'.format(regr.acc(X_data_test, y_data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
